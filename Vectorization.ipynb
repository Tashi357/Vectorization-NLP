{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f173f8a-7ac9-46b5-9182-229c50fc3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c53cf18-5510-4252-a36d-95630664ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/binodrai/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.', 'It integrates various domains such as statistics, machine learning, data mining, and big data analytics.', 'Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.', 'It involves data collection, data cleaning, data analysis, and data visualization.', 'Data scientists use programming languages like Python and R to perform data analysis.', 'They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.', 'The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.', 'By doing so, it helps organizations improve their operations and strategies.', 'Data science is a rapidly growing field, with increasing demand for skilled professionals.', 'It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.']\n",
      "Original Document: Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\n",
      "Text After HTML Removal: data science is an interdisciplinary field that uses scientific methods processes algorithms and systems to extract knowledge and insights from structured and unstructured data\n",
      "Tokens: ['data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', 'processes', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'structured', 'and', 'unstructured', 'data']\n",
      "Filtered Tokens: ['data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', 'processes', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'structured', 'unstructured', 'data']\n",
      "Stemmed Tokens: ['data', 'scienc', 'interdisciplinari', 'field', 'use', 'scientif', 'method', 'process', 'algorithm', 'system', 'extract', 'knowledg', 'insight', 'structur', 'unstructur', 'data']\n",
      "Lemmatized Tokens: ['data', 'science', 'interdisciplinary', 'field', 'us', 'scientific', 'method', 'process', 'algorithm', 'system', 'extract', 'knowledge', 'insight', 'structure', 'unstructured', 'data']\n",
      "--------------------------------------------------\n",
      "Original Document: It integrates various domains such as statistics, machine learning, data mining, and big data analytics.\n",
      "Text After HTML Removal: it integrates various domains such as statistics machine learning data mining and big data analytics\n",
      "Tokens: ['it', 'integrates', 'various', 'domains', 'such', 'as', 'statistics', 'machine', 'learning', 'data', 'mining', 'and', 'big', 'data', 'analytics']\n",
      "Filtered Tokens: ['integrates', 'various', 'domains', 'statistics', 'machine', 'learning', 'data', 'mining', 'big', 'data', 'analytics']\n",
      "Stemmed Tokens: ['integr', 'variou', 'domain', 'statist', 'machin', 'learn', 'data', 'mine', 'big', 'data', 'analyt']\n",
      "Lemmatized Tokens: ['integrates', 'various', 'domain', 'statistic', 'machine', 'learn', 'data', 'mining', 'big', 'data', 'analytics']\n",
      "--------------------------------------------------\n",
      "Original Document: Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.\n",
      "Text After HTML Removal: data science is essential for making informed decisions in business healthcare finance and many other industries\n",
      "Tokens: ['data', 'science', 'is', 'essential', 'for', 'making', 'informed', 'decisions', 'in', 'business', 'healthcare', 'finance', 'and', 'many', 'other', 'industries']\n",
      "Filtered Tokens: ['data', 'science', 'essential', 'making', 'informed', 'decisions', 'business', 'healthcare', 'finance', 'many', 'industries']\n",
      "Stemmed Tokens: ['data', 'scienc', 'essenti', 'make', 'inform', 'decis', 'busi', 'healthcar', 'financ', 'mani', 'industri']\n",
      "Lemmatized Tokens: ['data', 'science', 'essential', 'make', 'inform', 'decision', 'business', 'healthcare', 'finance', 'many', 'industry']\n",
      "--------------------------------------------------\n",
      "Original Document: It involves data collection, data cleaning, data analysis, and data visualization.\n",
      "Text After HTML Removal: it involves data collection data cleaning data analysis and data visualization\n",
      "Tokens: ['it', 'involves', 'data', 'collection', 'data', 'cleaning', 'data', 'analysis', 'and', 'data', 'visualization']\n",
      "Filtered Tokens: ['involves', 'data', 'collection', 'data', 'cleaning', 'data', 'analysis', 'data', 'visualization']\n",
      "Stemmed Tokens: ['involv', 'data', 'collect', 'data', 'clean', 'data', 'analysi', 'data', 'visual']\n",
      "Lemmatized Tokens: ['involves', 'data', 'collection', 'data', 'cleaning', 'data', 'analysis', 'data', 'visualization']\n",
      "--------------------------------------------------\n",
      "Original Document: Data scientists use programming languages like Python and R to perform data analysis.\n",
      "Text After HTML Removal: data scientists use programming languages like python and r to perform data analysis\n",
      "Tokens: ['data', 'scientists', 'use', 'programming', 'languages', 'like', 'python', 'and', 'r', 'to', 'perform', 'data', 'analysis']\n",
      "Filtered Tokens: ['data', 'scientists', 'use', 'programming', 'languages', 'like', 'python', 'r', 'perform', 'data', 'analysis']\n",
      "Stemmed Tokens: ['data', 'scientist', 'use', 'program', 'languag', 'like', 'python', 'r', 'perform', 'data', 'analysi']\n",
      "Lemmatized Tokens: ['data', 'scientist', 'use', 'program', 'language', 'like', 'python', 'r', 'perform', 'data', 'analysis']\n",
      "--------------------------------------------------\n",
      "Original Document: They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.\n",
      "Text After HTML Removal: they also employ tools such as hadoop spark and sql for handling large datasets\n",
      "Tokens: ['they', 'also', 'employ', 'tools', 'such', 'as', 'hadoop', 'spark', 'and', 'sql', 'for', 'handling', 'large', 'datasets']\n",
      "Filtered Tokens: ['also', 'employ', 'tools', 'hadoop', 'spark', 'sql', 'handling', 'large', 'datasets']\n",
      "Stemmed Tokens: ['also', 'employ', 'tool', 'hadoop', 'spark', 'sql', 'handl', 'larg', 'dataset']\n",
      "Lemmatized Tokens: ['also', 'employ', 'tool', 'hadoop', 'spark', 'sql', 'handle', 'large', 'datasets']\n",
      "--------------------------------------------------\n",
      "Original Document: The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.\n",
      "Text After HTML Removal: the ultimate goal of data science is to uncover hidden patterns correlations and trends\n",
      "Tokens: ['the', 'ultimate', 'goal', 'of', 'data', 'science', 'is', 'to', 'uncover', 'hidden', 'patterns', 'correlations', 'and', 'trends']\n",
      "Filtered Tokens: ['ultimate', 'goal', 'data', 'science', 'uncover', 'hidden', 'patterns', 'correlations', 'trends']\n",
      "Stemmed Tokens: ['ultim', 'goal', 'data', 'scienc', 'uncov', 'hidden', 'pattern', 'correl', 'trend']\n",
      "Lemmatized Tokens: ['ultimate', 'goal', 'data', 'science', 'uncover', 'hidden', 'pattern', 'correlation', 'trend']\n",
      "--------------------------------------------------\n",
      "Original Document: By doing so, it helps organizations improve their operations and strategies.\n",
      "Text After HTML Removal: by doing so it helps organizations improve their operations and strategies\n",
      "Tokens: ['by', 'doing', 'so', 'it', 'helps', 'organizations', 'improve', 'their', 'operations', 'and', 'strategies']\n",
      "Filtered Tokens: ['helps', 'organizations', 'improve', 'operations', 'strategies']\n",
      "Stemmed Tokens: ['help', 'organ', 'improv', 'oper', 'strategi']\n",
      "Lemmatized Tokens: ['help', 'organization', 'improve', 'operation', 'strategy']\n",
      "--------------------------------------------------\n",
      "Original Document: Data science is a rapidly growing field, with increasing demand for skilled professionals.\n",
      "Text After HTML Removal: data science is a rapidly growing field with increasing demand for skilled professionals\n",
      "Tokens: ['data', 'science', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'increasing', 'demand', 'for', 'skilled', 'professionals']\n",
      "Filtered Tokens: ['data', 'science', 'rapidly', 'growing', 'field', 'increasing', 'demand', 'skilled', 'professionals']\n",
      "Stemmed Tokens: ['data', 'scienc', 'rapidli', 'grow', 'field', 'increas', 'demand', 'skill', 'profession']\n",
      "Lemmatized Tokens: ['data', 'science', 'rapidly', 'grow', 'field', 'increase', 'demand', 'skilled', 'professional']\n",
      "--------------------------------------------------\n",
      "Original Document: It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.\n",
      "Text After HTML Removal: it is transforming the way we understand and interact with data leading to more accurate predictions and better decisionmaking\n",
      "Tokens: ['it', 'is', 'transforming', 'the', 'way', 'we', 'understand', 'and', 'interact', 'with', 'data', 'leading', 'to', 'more', 'accurate', 'predictions', 'and', 'better', 'decisionmaking']\n",
      "Filtered Tokens: ['transforming', 'way', 'understand', 'interact', 'data', 'leading', 'accurate', 'predictions', 'better', 'decisionmaking']\n",
      "Stemmed Tokens: ['transform', 'way', 'understand', 'interact', 'data', 'lead', 'accur', 'predict', 'better', 'decisionmak']\n",
      "Lemmatized Tokens: ['transform', 'way', 'understand', 'interact', 'data', 'lead', 'accurate', 'prediction', 'well', 'decisionmaking']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the Porter Stemmer and WordNet Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample corpus\n",
    "# Corrected corpus\n",
    "corpus = [\n",
    "    \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    \"It integrates various domains such as statistics, machine learning, data mining, and big data analytics.\",\n",
    "    \"Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.\",\n",
    "    \"It involves data collection, data cleaning, data analysis, and data visualization.\",\n",
    "    \"Data scientists use programming languages like Python and R to perform data analysis.\",\n",
    "    \"They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.\",\n",
    "    \"The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.\",\n",
    "    \"By doing so, it helps organizations improve their operations and strategies.\",\n",
    "    \"Data science is a rapidly growing field, with increasing demand for skilled professionals.\",\n",
    "    \"It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.\"\n",
    "]\n",
    "\n",
    "# Display the corrected corpus\n",
    "print(corpus)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokens': tokens,\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'stemmed_tokens': stemmed_tokens,\n",
    "        'lemmatized_tokens': lemmatized_tokens\n",
    "    }\n",
    "\n",
    "# Function to get the part of speech tag for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Process each document in the corpus\n",
    "for document in corpus:\n",
    "    result = preprocess_text(document)\n",
    "    print(\"Original Document:\", document)\n",
    "    print(\"Text After HTML Removal:\", result['original'])\n",
    "    print(\"Tokens:\", result['tokens'])\n",
    "    print(\"Filtered Tokens:\", result['filtered_tokens'])\n",
    "    print(\"Stemmed Tokens:\", result['stemmed_tokens'])\n",
    "    print(\"Lemmatized Tokens:\", result['lemmatized_tokens'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d34b719-6c46-46f7-b3b7-10500909ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Statement:\n",
      " 0    data science is an interdisciplinary field tha...\n",
      "1    it integrates various domains such as statisti...\n",
      "2    data science is essential for making informed ...\n",
      "3    it involves data collection, data cleaning, da...\n",
      "4    data scientists use programming languages like...\n",
      "5    they also employ tools such as hadoop, spark, ...\n",
      "6    the ultimate goal of data science is to uncove...\n",
      "7    by doing so, it helps organizations improve th...\n",
      "8    data science is a rapidly growing field, with ...\n",
      "9    it is transforming the way we understand and i...\n",
      "Name: Statement, dtype: object\n",
      "\n",
      "One-Hot Encoded:\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 1 1 1]]\n",
      "Vocabulary: ['accurate' 'algorithms' 'also' 'an' 'analysis' 'analytics' 'and' 'as'\n",
      " 'better' 'big' 'business' 'by' 'cleaning' 'collection' 'correlations'\n",
      " 'data' 'datasets' 'decision' 'decisions' 'demand' 'doing' 'domains'\n",
      " 'employ' 'essential' 'extract' 'field' 'finance' 'for' 'from' 'goal'\n",
      " 'growing' 'hadoop' 'handling' 'healthcare' 'helps' 'hidden' 'improve'\n",
      " 'in' 'increasing' 'industries' 'informed' 'insights' 'integrates'\n",
      " 'interact' 'interdisciplinary' 'involves' 'is' 'it' 'knowledge'\n",
      " 'languages' 'large' 'leading' 'learning' 'like' 'machine' 'making' 'many'\n",
      " 'methods' 'mining' 'more' 'of' 'operations' 'organizations' 'other'\n",
      " 'patterns' 'perform' 'predictions' 'processes' 'professionals'\n",
      " 'programming' 'python' 'rapidly' 'science' 'scientific' 'scientists'\n",
      " 'skilled' 'so' 'spark' 'sql' 'statistics' 'strategies' 'structured'\n",
      " 'such' 'systems' 'that' 'the' 'their' 'they' 'to' 'tools' 'transforming'\n",
      " 'trends' 'ultimate' 'uncover' 'understand' 'unstructured' 'use' 'uses'\n",
      " 'various' 'visualization' 'way' 'we' 'with']\n",
      "\n",
      "Bag of Words:\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 1 1 1]]\n",
      "Vocabulary: ['accurate' 'algorithms' 'also' 'an' 'analysis' 'analytics' 'and' 'as'\n",
      " 'better' 'big' 'business' 'by' 'cleaning' 'collection' 'correlations'\n",
      " 'data' 'datasets' 'decision' 'decisions' 'demand' 'doing' 'domains'\n",
      " 'employ' 'essential' 'extract' 'field' 'finance' 'for' 'from' 'goal'\n",
      " 'growing' 'hadoop' 'handling' 'healthcare' 'helps' 'hidden' 'improve'\n",
      " 'in' 'increasing' 'industries' 'informed' 'insights' 'integrates'\n",
      " 'interact' 'interdisciplinary' 'involves' 'is' 'it' 'knowledge'\n",
      " 'languages' 'large' 'leading' 'learning' 'like' 'machine' 'making' 'many'\n",
      " 'methods' 'mining' 'more' 'of' 'operations' 'organizations' 'other'\n",
      " 'patterns' 'perform' 'predictions' 'processes' 'professionals'\n",
      " 'programming' 'python' 'rapidly' 'science' 'scientific' 'scientists'\n",
      " 'skilled' 'so' 'spark' 'sql' 'statistics' 'strategies' 'structured'\n",
      " 'such' 'systems' 'that' 'the' 'their' 'they' 'to' 'tools' 'transforming'\n",
      " 'trends' 'ultimate' 'uncover' 'understand' 'unstructured' 'use' 'uses'\n",
      " 'various' 'visualization' 'way' 'we' 'with']\n",
      "\n",
      "TF-IDF:\n",
      " [[0.         0.22813529 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.28359471]\n",
      " [0.25609006 0.         0.         ... 0.25609006 0.25609006 0.21769994]]\n",
      "Vocabulary: ['accurate' 'algorithms' 'also' 'an' 'analysis' 'analytics' 'and' 'as'\n",
      " 'better' 'big' 'business' 'by' 'cleaning' 'collection' 'correlations'\n",
      " 'data' 'datasets' 'decision' 'decisions' 'demand' 'doing' 'domains'\n",
      " 'employ' 'essential' 'extract' 'field' 'finance' 'for' 'from' 'goal'\n",
      " 'growing' 'hadoop' 'handling' 'healthcare' 'helps' 'hidden' 'improve'\n",
      " 'in' 'increasing' 'industries' 'informed' 'insights' 'integrates'\n",
      " 'interact' 'interdisciplinary' 'involves' 'is' 'it' 'knowledge'\n",
      " 'languages' 'large' 'leading' 'learning' 'like' 'machine' 'making' 'many'\n",
      " 'methods' 'mining' 'more' 'of' 'operations' 'organizations' 'other'\n",
      " 'patterns' 'perform' 'predictions' 'processes' 'professionals'\n",
      " 'programming' 'python' 'rapidly' 'science' 'scientific' 'scientists'\n",
      " 'skilled' 'so' 'spark' 'sql' 'statistics' 'strategies' 'structured'\n",
      " 'such' 'systems' 'that' 'the' 'their' 'they' 'to' 'tools' 'transforming'\n",
      " 'trends' 'ultimate' 'uncover' 'understand' 'unstructured' 'use' 'uses'\n",
      " 'various' 'visualization' 'way' 'we' 'with']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample reviews\n",
    "data = {\n",
    "   \"Statement\": [\n",
    "    \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    \"It integrates various domains such as statistics, machine learning, data mining, and big data analytics.\",\n",
    "    \"Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.\",\n",
    "    \"It involves data collection, data cleaning, data analysis, and data visualization.\",\n",
    "    \"Data scientists use programming languages like Python and R to perform data analysis.\",\n",
    "    \"They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.\",\n",
    "    \"The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.\",\n",
    "    \"By doing so, it helps organizations improve their operations and strategies.\",\n",
    "    \"Data science is a rapidly growing field, with increasing demand for skilled professionals.\",\n",
    "    \"It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.\"\n",
    "]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert to lowercase\n",
    "df['Statement'] = df['Statement'].str.lower()\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = CountVectorizer(binary=True)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(df['Statement'])\n",
    "\n",
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_encoded = count_vectorizer.fit_transform(df['Statement'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded = tfidf_vectorizer.fit_transform(df['Statement'])\n",
    "\n",
    "# Output Results\n",
    "print(\"Original Statement:\\n\", df['Statement'])\n",
    "print(\"\\nOne-Hot Encoded:\\n\", one_hot_encoded.toarray())\n",
    "print(\"Vocabulary:\", one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBag of Words:\\n\", bow_encoded.toarray())\n",
    "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_encoded.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4371e3d2-54aa-451a-9f4c-4ddc453d7a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Statement:\n",
      " 0    data science is an interdisciplinary field tha...\n",
      "1    it integrates various domains such as statisti...\n",
      "2    data science is essential for making informed ...\n",
      "3    it involves data collection, data cleaning, da...\n",
      "4    data scientists use programming languages like...\n",
      "5    they also employ tools such as hadoop, spark, ...\n",
      "6    the ultimate goal of data science is to uncove...\n",
      "7    by doing so, it helps organizations improve th...\n",
      "8    data science is a rapidly growing field, with ...\n",
      "9    it is transforming the way we understand and i...\n",
      "Name: Statement, dtype: object\n",
      "\n",
      "One-Hot Encoded:\n",
      "    accurate  algorithms  also  an  analysis  analytics  and  as  better  big  \\\n",
      "0         0           1     0   1         0          0    1   0       0    0   \n",
      "1         0           0     0   0         0          1    1   1       0    1   \n",
      "2         0           0     0   0         0          0    1   0       0    0   \n",
      "3         0           0     0   0         1          0    1   0       0    0   \n",
      "4         0           0     0   0         1          0    1   0       0    0   \n",
      "5         0           0     1   0         0          0    1   1       0    0   \n",
      "6         0           0     0   0         0          0    1   0       0    0   \n",
      "7         0           0     0   0         0          0    1   0       0    0   \n",
      "8         0           0     0   0         0          0    0   0       0    0   \n",
      "9         1           0     0   0         0          0    1   0       1    0   \n",
      "\n",
      "   ...  uncover  understand  unstructured  use  uses  various  visualization  \\\n",
      "0  ...        0           0             1    0     1        0              0   \n",
      "1  ...        0           0             0    0     0        1              0   \n",
      "2  ...        0           0             0    0     0        0              0   \n",
      "3  ...        0           0             0    0     0        0              1   \n",
      "4  ...        0           0             0    1     0        0              0   \n",
      "5  ...        0           0             0    0     0        0              0   \n",
      "6  ...        1           0             0    0     0        0              0   \n",
      "7  ...        0           0             0    0     0        0              0   \n",
      "8  ...        0           0             0    0     0        0              0   \n",
      "9  ...        0           1             0    0     0        0              0   \n",
      "\n",
      "   way  we  with  \n",
      "0    0   0     0  \n",
      "1    0   0     0  \n",
      "2    0   0     0  \n",
      "3    0   0     0  \n",
      "4    0   0     0  \n",
      "5    0   0     0  \n",
      "6    0   0     0  \n",
      "7    0   0     0  \n",
      "8    0   0     1  \n",
      "9    1   1     1  \n",
      "\n",
      "[10 rows x 103 columns]\n",
      "\n",
      "Bag of Words:\n",
      "    accurate  algorithms  also  an  analysis  analytics  and  as  better  big  \\\n",
      "0         0           1     0   1         0          0    3   0       0    0   \n",
      "1         0           0     0   0         0          1    1   1       0    1   \n",
      "2         0           0     0   0         0          0    1   0       0    0   \n",
      "3         0           0     0   0         1          0    1   0       0    0   \n",
      "4         0           0     0   0         1          0    1   0       0    0   \n",
      "5         0           0     1   0         0          0    1   1       0    0   \n",
      "6         0           0     0   0         0          0    1   0       0    0   \n",
      "7         0           0     0   0         0          0    1   0       0    0   \n",
      "8         0           0     0   0         0          0    0   0       0    0   \n",
      "9         1           0     0   0         0          0    2   0       1    0   \n",
      "\n",
      "   ...  uncover  understand  unstructured  use  uses  various  visualization  \\\n",
      "0  ...        0           0             1    0     1        0              0   \n",
      "1  ...        0           0             0    0     0        1              0   \n",
      "2  ...        0           0             0    0     0        0              0   \n",
      "3  ...        0           0             0    0     0        0              1   \n",
      "4  ...        0           0             0    1     0        0              0   \n",
      "5  ...        0           0             0    0     0        0              0   \n",
      "6  ...        1           0             0    0     0        0              0   \n",
      "7  ...        0           0             0    0     0        0              0   \n",
      "8  ...        0           0             0    0     0        0              0   \n",
      "9  ...        0           1             0    0     0        0              0   \n",
      "\n",
      "   way  we  with  \n",
      "0    0   0     0  \n",
      "1    0   0     0  \n",
      "2    0   0     0  \n",
      "3    0   0     0  \n",
      "4    0   0     0  \n",
      "5    0   0     0  \n",
      "6    0   0     0  \n",
      "7    0   0     0  \n",
      "8    0   0     1  \n",
      "9    1   1     1  \n",
      "\n",
      "[10 rows x 103 columns]\n",
      "\n",
      "TF-IDF:\n",
      "    accurate  algorithms      also        an  analysis  analytics       and  \\\n",
      "0   0.00000    0.228135  0.000000  0.228135  0.000000   0.000000  0.277156   \n",
      "1   0.00000    0.000000  0.000000  0.000000  0.000000   0.290683  0.117715   \n",
      "2   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.114877   \n",
      "3   0.00000    0.000000  0.000000  0.000000  0.291978   0.000000  0.139090   \n",
      "4   0.00000    0.000000  0.000000  0.000000  0.281615   0.000000  0.134153   \n",
      "5   0.00000    0.000000  0.286741  0.000000  0.000000   0.000000  0.116118   \n",
      "6   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.126114   \n",
      "7   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.130691   \n",
      "8   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
      "9   0.25609    0.000000  0.000000  0.000000  0.000000   0.000000  0.207412   \n",
      "\n",
      "         as   better       big  ...   uncover  understand  unstructured  \\\n",
      "0  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.228135   \n",
      "1  0.247107  0.00000  0.290683  ...  0.000000     0.00000      0.000000   \n",
      "2  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
      "3  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
      "4  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
      "5  0.243756  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
      "6  0.000000  0.00000  0.000000  ...  0.311426     0.00000      0.000000   \n",
      "7  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
      "8  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
      "9  0.000000  0.25609  0.000000  ...  0.000000     0.25609      0.000000   \n",
      "\n",
      "        use      uses   various  visualization      way       we      with  \n",
      "0  0.000000  0.228135  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
      "1  0.000000  0.000000  0.290683       0.000000  0.00000  0.00000  0.000000  \n",
      "2  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
      "3  0.000000  0.000000  0.000000       0.343466  0.00000  0.00000  0.000000  \n",
      "4  0.331277  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
      "5  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
      "6  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
      "7  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
      "8  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.283595  \n",
      "9  0.000000  0.000000  0.000000       0.000000  0.25609  0.25609  0.217700  \n",
      "\n",
      "[10 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample reviews\n",
    "data = {\n",
    "   \"Statement\": [\n",
    "    \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    \"It integrates various domains such as statistics, machine learning, data mining, and big data analytics.\",\n",
    "    \"Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.\",\n",
    "    \"It involves data collection, data cleaning, data analysis, and data visualization.\",\n",
    "    \"Data scientists use programming languages like Python and R to perform data analysis.\",\n",
    "    \"They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.\",\n",
    "    \"The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.\",\n",
    "    \"By doing so, it helps organizations improve their operations and strategies.\",\n",
    "    \"Data science is a rapidly growing field, with increasing demand for skilled professionals.\",\n",
    "    \"It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.\"\n",
    "]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert to lowercase\n",
    "df['Statement'] = df['Statement'].str.lower()\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = CountVectorizer(binary=True)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(df['Statement'])\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded.toarray(), columns=one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_encoded = count_vectorizer.fit_transform(df['Statement'])\n",
    "bow_df = pd.DataFrame(bow_encoded.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded = tfidf_vectorizer.fit_transform(df['Statement'])\n",
    "tfidf_df = pd.DataFrame(tfidf_encoded.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Output Results\n",
    "print(\"Original Statement:\\n\", df['Statement'])\n",
    "print(\"\\nOne-Hot Encoded:\\n\", one_hot_df)\n",
    "print(\"\\nBag of Words:\\n\", bow_df)\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52679fd-2c9d-4c3b-bc3c-bcfad0dfea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Statement:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    data science is an interdisciplinary field tha...\n",
       "1    it integrates various domains such as statisti...\n",
       "2    data science is essential for making informed ...\n",
       "3    it involves data collection, data cleaning, da...\n",
       "4    data scientists use programming languages like...\n",
       "5    they also employ tools such as hadoop, spark, ...\n",
       "6    the ultimate goal of data science is to uncove...\n",
       "7    by doing so, it helps organizations improve th...\n",
       "8    data science is a rapidly growing field, with ...\n",
       "9    it is transforming the way we understand and i...\n",
       "Name: Statement, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot Encoded:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accurate</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>better</th>\n",
       "      <th>big</th>\n",
       "      <th>...</th>\n",
       "      <th>uncover</th>\n",
       "      <th>understand</th>\n",
       "      <th>unstructured</th>\n",
       "      <th>use</th>\n",
       "      <th>uses</th>\n",
       "      <th>various</th>\n",
       "      <th>visualization</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accurate  algorithms  also  an  analysis  analytics  and  as  better  big  \\\n",
       "0         0           1     0   1         0          0    1   0       0    0   \n",
       "1         0           0     0   0         0          1    1   1       0    1   \n",
       "2         0           0     0   0         0          0    1   0       0    0   \n",
       "3         0           0     0   0         1          0    1   0       0    0   \n",
       "4         0           0     0   0         1          0    1   0       0    0   \n",
       "5         0           0     1   0         0          0    1   1       0    0   \n",
       "6         0           0     0   0         0          0    1   0       0    0   \n",
       "7         0           0     0   0         0          0    1   0       0    0   \n",
       "8         0           0     0   0         0          0    0   0       0    0   \n",
       "9         1           0     0   0         0          0    1   0       1    0   \n",
       "\n",
       "   ...  uncover  understand  unstructured  use  uses  various  visualization  \\\n",
       "0  ...        0           0             1    0     1        0              0   \n",
       "1  ...        0           0             0    0     0        1              0   \n",
       "2  ...        0           0             0    0     0        0              0   \n",
       "3  ...        0           0             0    0     0        0              1   \n",
       "4  ...        0           0             0    1     0        0              0   \n",
       "5  ...        0           0             0    0     0        0              0   \n",
       "6  ...        1           0             0    0     0        0              0   \n",
       "7  ...        0           0             0    0     0        0              0   \n",
       "8  ...        0           0             0    0     0        0              0   \n",
       "9  ...        0           1             0    0     0        0              0   \n",
       "\n",
       "   way  we  with  \n",
       "0    0   0     0  \n",
       "1    0   0     0  \n",
       "2    0   0     0  \n",
       "3    0   0     0  \n",
       "4    0   0     0  \n",
       "5    0   0     0  \n",
       "6    0   0     0  \n",
       "7    0   0     0  \n",
       "8    0   0     1  \n",
       "9    1   1     1  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accurate</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>better</th>\n",
       "      <th>big</th>\n",
       "      <th>...</th>\n",
       "      <th>uncover</th>\n",
       "      <th>understand</th>\n",
       "      <th>unstructured</th>\n",
       "      <th>use</th>\n",
       "      <th>uses</th>\n",
       "      <th>various</th>\n",
       "      <th>visualization</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accurate  algorithms  also  an  analysis  analytics  and  as  better  big  \\\n",
       "0         0           1     0   1         0          0    3   0       0    0   \n",
       "1         0           0     0   0         0          1    1   1       0    1   \n",
       "2         0           0     0   0         0          0    1   0       0    0   \n",
       "3         0           0     0   0         1          0    1   0       0    0   \n",
       "4         0           0     0   0         1          0    1   0       0    0   \n",
       "5         0           0     1   0         0          0    1   1       0    0   \n",
       "6         0           0     0   0         0          0    1   0       0    0   \n",
       "7         0           0     0   0         0          0    1   0       0    0   \n",
       "8         0           0     0   0         0          0    0   0       0    0   \n",
       "9         1           0     0   0         0          0    2   0       1    0   \n",
       "\n",
       "   ...  uncover  understand  unstructured  use  uses  various  visualization  \\\n",
       "0  ...        0           0             1    0     1        0              0   \n",
       "1  ...        0           0             0    0     0        1              0   \n",
       "2  ...        0           0             0    0     0        0              0   \n",
       "3  ...        0           0             0    0     0        0              1   \n",
       "4  ...        0           0             0    1     0        0              0   \n",
       "5  ...        0           0             0    0     0        0              0   \n",
       "6  ...        1           0             0    0     0        0              0   \n",
       "7  ...        0           0             0    0     0        0              0   \n",
       "8  ...        0           0             0    0     0        0              0   \n",
       "9  ...        0           1             0    0     0        0              0   \n",
       "\n",
       "   way  we  with  \n",
       "0    0   0     0  \n",
       "1    0   0     0  \n",
       "2    0   0     0  \n",
       "3    0   0     0  \n",
       "4    0   0     0  \n",
       "5    0   0     0  \n",
       "6    0   0     0  \n",
       "7    0   0     0  \n",
       "8    0   0     1  \n",
       "9    1   1     1  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accurate</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>better</th>\n",
       "      <th>big</th>\n",
       "      <th>...</th>\n",
       "      <th>uncover</th>\n",
       "      <th>understand</th>\n",
       "      <th>unstructured</th>\n",
       "      <th>use</th>\n",
       "      <th>uses</th>\n",
       "      <th>various</th>\n",
       "      <th>visualization</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.228135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.228135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290683</td>\n",
       "      <td>0.117715</td>\n",
       "      <td>0.247107</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.290683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343466</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116118</td>\n",
       "      <td>0.243756</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311426</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.283595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25609</td>\n",
       "      <td>0.25609</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accurate  algorithms      also        an  analysis  analytics       and  \\\n",
       "0   0.00000    0.228135  0.000000  0.228135  0.000000   0.000000  0.277156   \n",
       "1   0.00000    0.000000  0.000000  0.000000  0.000000   0.290683  0.117715   \n",
       "2   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.114877   \n",
       "3   0.00000    0.000000  0.000000  0.000000  0.291978   0.000000  0.139090   \n",
       "4   0.00000    0.000000  0.000000  0.000000  0.281615   0.000000  0.134153   \n",
       "5   0.00000    0.000000  0.286741  0.000000  0.000000   0.000000  0.116118   \n",
       "6   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.126114   \n",
       "7   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.130691   \n",
       "8   0.00000    0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "9   0.25609    0.000000  0.000000  0.000000  0.000000   0.000000  0.207412   \n",
       "\n",
       "         as   better       big  ...   uncover  understand  unstructured  \\\n",
       "0  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.228135   \n",
       "1  0.247107  0.00000  0.290683  ...  0.000000     0.00000      0.000000   \n",
       "2  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
       "3  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
       "4  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
       "5  0.243756  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
       "6  0.000000  0.00000  0.000000  ...  0.311426     0.00000      0.000000   \n",
       "7  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
       "8  0.000000  0.00000  0.000000  ...  0.000000     0.00000      0.000000   \n",
       "9  0.000000  0.25609  0.000000  ...  0.000000     0.25609      0.000000   \n",
       "\n",
       "        use      uses   various  visualization      way       we      with  \n",
       "0  0.000000  0.228135  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
       "1  0.000000  0.000000  0.290683       0.000000  0.00000  0.00000  0.000000  \n",
       "2  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
       "3  0.000000  0.000000  0.000000       0.343466  0.00000  0.00000  0.000000  \n",
       "4  0.331277  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
       "5  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
       "6  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
       "7  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.000000  \n",
       "8  0.000000  0.000000  0.000000       0.000000  0.00000  0.00000  0.283595  \n",
       "9  0.000000  0.000000  0.000000       0.000000  0.25609  0.25609  0.217700  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(\"Original Statement:\\n\")\n",
    "display(df['Statement'])\n",
    "\n",
    "print(\"\\nOne-Hot Encoded:\\n\")\n",
    "display(one_hot_df)\n",
    "\n",
    "print(\"\\nBag of Words:\\n\")\n",
    "display(bow_df)\n",
    "\n",
    "print(\"\\nTF-IDF:\\n\")\n",
    "display(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4663c-e9bd-49f5-9ae0-ff4c4050659e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
